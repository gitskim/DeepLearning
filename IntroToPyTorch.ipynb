{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntroToPyTorch",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlHjkcDCjyYH",
        "colab_type": "text"
      },
      "source": [
        "# Tensors\n",
        "\n",
        "* A generalization of matrices\n",
        "\n",
        "## Vector\n",
        "\n",
        "* One dimensional tensors\n",
        "\n",
        "## Matrix\n",
        "\n",
        "* 2-dimensional tensors\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLO4lE2okuZj",
        "colab_type": "text"
      },
      "source": [
        "# Basic Single Layer Neural Networks With Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku7feFA3QFje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqTHvq84k50Z",
        "colab_type": "code",
        "outputId": "3780ed41-acd2-4d06-da5f-e642d2c550bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# To get the output as a probability\n",
        "def sigmoid_activation(x):\n",
        "  return 1/(1 + torch.exp(-x))\n",
        "\n",
        "\n",
        "# Set the random seed so things are predictable\n",
        "torch.manual_seed(7)\n",
        "\n",
        "# Features are 5 random normal variable\n",
        "# 1 row and 5 columns \n",
        "features = torch.randn((1, 5))\n",
        "\n",
        "# create another tensor with the same shape as input\n",
        "weights = torch.randn_like(features)\n",
        "\n",
        "bias = torch.randn((1, 1))\n",
        "\n",
        "print(features, weights, bias)\n",
        "\n",
        "# To calculate the output of this network using the weights and bias\n",
        "''' \n",
        "NOTE:\n",
        "\n",
        "torch.mm is more strict about the input shape as opposed to torch.matmul, which supports broadcasting, being able to give you a wrong output. \n",
        "\n",
        "\n",
        "* weights.reshape(a, b) - a new tensor with the same data as weights, but sometimes the actual data in memory isn't being changed and some other times it gives you a clone.\n",
        "* weights.reshape_(a, b) - the underscore is an in-place operation. It's changing only the tensor in memory without touching the data. \n",
        "  If you request the size that is more or less than the original shape, you can cut off the\n",
        "  original data\n",
        "* weights.view(a, b) - always returns a new data without touching any of the data in memory. For an impossible shape, it returns an error. \n",
        "\n",
        "torch.mm(features, weights.view(5, 1) is equivalent to torch.sum(features * weights) and (features * weights).sum(), \n",
        "only if it's using vectors not matrices, but using torch function is most efficient.\n",
        "\n",
        "'''\n",
        "result = sigmoid_activation(torch.mm(features, weights.view(5, 1)) + bias)\n",
        "print(result)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]]) tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]]) tensor([[0.3177]])\n",
            "tensor([[0.1595]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZoPykoEvonD",
        "colab_type": "text"
      },
      "source": [
        "# Basic Multilayer Neural Network With Matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phgnhkonvuBu",
        "colab_type": "code",
        "outputId": "04ca6876-a70f-4322-82a9-9a76c1525d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.manual_seed(7)\n",
        "\n",
        "features = torch.randn((1, 3))\n",
        "n_input = features.shape[1]\n",
        "n_hidden = 2\n",
        "n_output = 1\n",
        "\n",
        "# question: why does it not have (), but B1's do?\n",
        "W1 = torch.randn((n_input, n_hidden))\n",
        "W2 = torch.randn((n_hidden, n_output))\n",
        "\n",
        "B1 = torch.randn((1, n_hidden))\n",
        "B2 = torch.randn((1, n_output))\n",
        "\n",
        "hidden_output = sigmoid_activation(torch.mm(features, W1) + B1)\n",
        "output = sigmoid_activation(torch.mm(hidden_output, W2) + B2)\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3171]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKKs2hJKyiBV",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network W/ MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp4VuNwIyhM7",
        "colab_type": "code",
        "outputId": "c68bb5fc-6d55-4e4a-94b1-5d0dd788a2fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import helper\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "'''\n",
        "Transforms: it does image transformations\n",
        "\n",
        "Compose: common image transformations can be chanined together using Compose.\n",
        "'''\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('./', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "!wget https://github.com/gitskim/Pasion/raw/master/diving.mat\n",
        "!wget https://raw.githubusercontent.com/gitskim/Pasion/master/Diving.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:03, 2881132.24it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 58144.95it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 972982.72it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21434.93it/s]            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 4304796.61it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 57898.21it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 875109.00it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21820.85it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "--2019-08-19 10:46:43--  https://github.com/gitskim/Pasion/raw/master/diving.mat\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/gitskim/Pasion/master/diving.mat [following]\n",
            "--2019-08-19 10:46:44--  https://raw.githubusercontent.com/gitskim/Pasion/master/diving.mat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9591612 (9.1M) [application/octet-stream]\n",
            "Saving to: ‘diving.mat’\n",
            "\n",
            "diving.mat          100%[===================>]   9.15M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-08-19 10:46:46 (67.1 MB/s) - ‘diving.mat’ saved [9591612/9591612]\n",
            "\n",
            "--2019-08-19 10:46:48--  https://raw.githubusercontent.com/gitskim/Pasion/master/Diving.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6139 (6.0K) [text/plain]\n",
            "Saving to: ‘Diving.txt’\n",
            "\n",
            "Diving.txt          100%[===================>]   6.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-08-19 10:46:48 (67.7 MB/s) - ‘Diving.txt’ saved [6139/6139]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Qt2mK3NkuJ",
        "colab_type": "code",
        "outputId": "ce21aab1-10fc-49c8-ff58-d66707c374a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "print(type(images))\n",
        "print(type(labels))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJnL8kECPRR4",
        "colab_type": "code",
        "outputId": "75ff440e-c755-4fe3-9bc8-b111f1f87030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 253,
              "height": 250
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9Hc5IVLPbwP",
        "colab_type": "code",
        "outputId": "b644eb43-ec84-4a66-cd5b-56db43e4f7b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "# flatten the input images\n",
        "# question: how does it pack and unpack using view, so that it retreives exactly same image?\n",
        "reshaped_images = images.view(64, 784)\n",
        "print(reshaped_images.shape)\n",
        "plt.imshow(reshaped_images[1].view(28, 28).numpy(), cmap='Greys_r');\n",
        "\n",
        "input_size = 64\n",
        "input_dim = 784\n",
        "hidden_state_size = 256\n",
        "output_size = 10\n",
        "W1 = torch.randn((input_dim, hidden_state_size))\n",
        "# Question: how can you create a bias vector not a matrix to add to the matrix?\n",
        "B1 = torch.randn(hidden_state_size)\n",
        "\n",
        "W2 = torch.randn((hidden_state_size, output_size))\n",
        "B2 = torch.randn((output_size))\n",
        "\n",
        "W1_calc = sigmoid_activation(torch.mm(reshaped_images, W1))\n",
        "\n",
        "result = sigmoid_activation(torch.mm(W1_calc, W2) + B2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 253,
              "height": 250
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtrG-Zh8riOE",
        "colab_type": "text"
      },
      "source": [
        "* if no dimension is mentioned, it will sum every value in the matrix resulting in one value. \n",
        "* if dimension is 0, it sums over rows, so the result is a vector with total column number of entries.\n",
        "Then each row will be divided by a different number.\n",
        "* if dimension is 1, it sums over columns, so the result is a vector with total row number of entries. Then each column will **not be able to be divided by a different number** this column vector, because PyTorch doesn't enable division across rows, but only columns by default, which is called **broadcasting**. If you want to divide a matrix by a column vector, it has to be modified  to be a matrix. e.g. (9) should be modified to (9, 1) to divide the matrix of (9, anyint)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBxLO08GrfWe",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SUX01UXgYIV",
        "colab_type": "code",
        "outputId": "640e21bc-c927-48ab-cb91-4e1cc9558537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# see how summing in different dimension results into a different result\n",
        "arr = np.array([[2, 3, 5], [5, 3, 2]], dtype=np.float)\n",
        "arr = torch.tensor(arr)\n",
        "print(arr)\n",
        "\n",
        "msum = torch.sum(arr, dim=0)\n",
        "print(msum.shape)\n",
        "res = arr / msum\n",
        "print(res)\n",
        "\n",
        "# softmax\n",
        "def softmax(x):\n",
        "  print(\"inside softmax: %d\", x.shape)\n",
        "  denominator = torch.sum(torch.exp(x), dim=1)\n",
        "  denominator_real = denominator.view(-1, 1)\n",
        "  \n",
        "  # question: what is the difference between (64) and (64, 1) visually?\n",
        "  \n",
        "  return torch.exp(x) / denominator_real\n",
        "  \n",
        "  \n",
        "prob = softmax(result)\n",
        "print(prob)\n",
        "# do double check by summing over each row and making sure it is 1. \n",
        "print(torch.sum(prob, dim=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 3., 5.],\n",
            "        [5., 3., 2.]], dtype=torch.float64)\n",
            "torch.Size([3])\n",
            "tensor([[0.2857, 0.5000, 0.7143],\n",
            "        [0.7143, 0.5000, 0.2857]], dtype=torch.float64)\n",
            "inside softmax: %d torch.Size([64, 10])\n",
            "tensor([[0.1328, 0.1328, 0.0489, 0.0562, 0.0489, 0.1328, 0.0490, 0.1328, 0.1328,\n",
            "         0.1328],\n",
            "        [0.1244, 0.1362, 0.0501, 0.0504, 0.0501, 0.1362, 0.0503, 0.1307, 0.1355,\n",
            "         0.1362],\n",
            "        [0.1327, 0.1327, 0.0488, 0.0594, 0.0500, 0.1327, 0.0488, 0.1295, 0.1327,\n",
            "         0.1327],\n",
            "        [0.1491, 0.1491, 0.0548, 0.0551, 0.0549, 0.1491, 0.0549, 0.1285, 0.0555,\n",
            "         0.1491],\n",
            "        [0.1146, 0.1129, 0.0650, 0.1146, 0.0939, 0.1146, 0.0426, 0.1146, 0.1146,\n",
            "         0.1127],\n",
            "        [0.1238, 0.1239, 0.0456, 0.1195, 0.0456, 0.1238, 0.0497, 0.1204, 0.1239,\n",
            "         0.1239],\n",
            "        [0.1305, 0.1305, 0.0480, 0.1272, 0.0481, 0.1305, 0.0545, 0.0696, 0.1305,\n",
            "         0.1305],\n",
            "        [0.1065, 0.1065, 0.1047, 0.1065, 0.0449, 0.1065, 0.1050, 0.1065, 0.1065,\n",
            "         0.1062],\n",
            "        [0.1168, 0.1277, 0.0470, 0.1245, 0.0482, 0.1277, 0.0471, 0.1246, 0.1088,\n",
            "         0.1277],\n",
            "        [0.1212, 0.1212, 0.0446, 0.0944, 0.0447, 0.1212, 0.1147, 0.0956, 0.1212,\n",
            "         0.1212],\n",
            "        [0.1230, 0.1237, 0.0456, 0.1192, 0.0459, 0.1240, 0.0464, 0.1240, 0.1240,\n",
            "         0.1240],\n",
            "        [0.1097, 0.1096, 0.0901, 0.1053, 0.1064, 0.1097, 0.0403, 0.1097, 0.1097,\n",
            "         0.1097],\n",
            "        [0.1242, 0.1246, 0.0459, 0.1240, 0.0506, 0.1247, 0.0459, 0.1129, 0.1225,\n",
            "         0.1247],\n",
            "        [0.1174, 0.1114, 0.1169, 0.1174, 0.0433, 0.1174, 0.0611, 0.0802, 0.1174,\n",
            "         0.1174],\n",
            "        [0.1243, 0.1244, 0.0458, 0.1244, 0.0474, 0.1244, 0.0463, 0.1139, 0.1244,\n",
            "         0.1244],\n",
            "        [0.1197, 0.1185, 0.0441, 0.1196, 0.0772, 0.1198, 0.0441, 0.1179, 0.1194,\n",
            "         0.1198],\n",
            "        [0.1264, 0.1256, 0.0466, 0.1059, 0.0605, 0.1264, 0.0466, 0.1093, 0.1264,\n",
            "         0.1264],\n",
            "        [0.1388, 0.0550, 0.1387, 0.0956, 0.0514, 0.1388, 0.0511, 0.0533, 0.1388,\n",
            "         0.1385],\n",
            "        [0.1243, 0.1243, 0.0457, 0.1193, 0.0458, 0.1243, 0.0457, 0.1219, 0.1243,\n",
            "         0.1243],\n",
            "        [0.1299, 0.1299, 0.0478, 0.0479, 0.1226, 0.1299, 0.0478, 0.0846, 0.1296,\n",
            "         0.1299],\n",
            "        [0.1236, 0.1236, 0.0510, 0.1216, 0.0455, 0.1236, 0.0455, 0.1236, 0.1183,\n",
            "         0.1236],\n",
            "        [0.1342, 0.1338, 0.0495, 0.1218, 0.0494, 0.1342, 0.0494, 0.0592, 0.1342,\n",
            "         0.1342],\n",
            "        [0.1277, 0.1277, 0.0470, 0.0974, 0.0471, 0.1277, 0.0470, 0.1243, 0.1262,\n",
            "         0.1277],\n",
            "        [0.1287, 0.1287, 0.0474, 0.0858, 0.0474, 0.1285, 0.0474, 0.1287, 0.1287,\n",
            "         0.1286],\n",
            "        [0.1330, 0.1335, 0.0492, 0.0495, 0.0541, 0.1317, 0.0491, 0.1334, 0.1332,\n",
            "         0.1334],\n",
            "        [0.1328, 0.1328, 0.0489, 0.1094, 0.0489, 0.1328, 0.0489, 0.0829, 0.1328,\n",
            "         0.1298],\n",
            "        [0.1212, 0.1244, 0.0458, 0.1241, 0.0458, 0.1244, 0.0461, 0.1195, 0.1243,\n",
            "         0.1244],\n",
            "        [0.1202, 0.1213, 0.0457, 0.1208, 0.0653, 0.1213, 0.0446, 0.1212, 0.1203,\n",
            "         0.1193],\n",
            "        [0.1239, 0.1235, 0.0457, 0.1235, 0.0457, 0.1239, 0.0456, 0.1203, 0.1239,\n",
            "         0.1239],\n",
            "        [0.1338, 0.1338, 0.0492, 0.0493, 0.0493, 0.1338, 0.0492, 0.1338, 0.1338,\n",
            "         0.1338],\n",
            "        [0.1345, 0.1345, 0.0495, 0.0517, 0.0518, 0.1345, 0.0495, 0.1255, 0.1340,\n",
            "         0.1345],\n",
            "        [0.1150, 0.1151, 0.1133, 0.0424, 0.1127, 0.1151, 0.0424, 0.1139, 0.1151,\n",
            "         0.1150],\n",
            "        [0.1176, 0.1176, 0.0460, 0.0882, 0.0434, 0.1176, 0.1172, 0.1176, 0.1176,\n",
            "         0.1171],\n",
            "        [0.1224, 0.1226, 0.0451, 0.1226, 0.0624, 0.1226, 0.0451, 0.1123, 0.1223,\n",
            "         0.1226],\n",
            "        [0.1201, 0.1201, 0.0442, 0.1200, 0.0715, 0.1201, 0.0442, 0.1198, 0.1201,\n",
            "         0.1201],\n",
            "        [0.1404, 0.1438, 0.0529, 0.0554, 0.0529, 0.1395, 0.0646, 0.0630, 0.1436,\n",
            "         0.1438],\n",
            "        [0.1217, 0.1227, 0.0451, 0.1226, 0.1037, 0.1227, 0.0451, 0.0709, 0.1227,\n",
            "         0.1227],\n",
            "        [0.1232, 0.1251, 0.0956, 0.0610, 0.0491, 0.1251, 0.0460, 0.1251, 0.1246,\n",
            "         0.1251],\n",
            "        [0.1270, 0.1270, 0.0470, 0.1270, 0.0468, 0.1270, 0.0467, 0.1259, 0.0984,\n",
            "         0.1270],\n",
            "        [0.1155, 0.1155, 0.0426, 0.1107, 0.0426, 0.1155, 0.1136, 0.1132, 0.1154,\n",
            "         0.1155],\n",
            "        [0.1221, 0.1238, 0.0468, 0.1211, 0.0455, 0.1238, 0.0455, 0.1238, 0.1238,\n",
            "         0.1238],\n",
            "        [0.1266, 0.1266, 0.0466, 0.0501, 0.0466, 0.1266, 0.1059, 0.1266, 0.1265,\n",
            "         0.1180],\n",
            "        [0.1273, 0.1273, 0.0468, 0.1214, 0.0468, 0.1273, 0.0468, 0.1017, 0.1273,\n",
            "         0.1273],\n",
            "        [0.1292, 0.1292, 0.0475, 0.1185, 0.0476, 0.1292, 0.0475, 0.1289, 0.1292,\n",
            "         0.0930],\n",
            "        [0.1142, 0.1145, 0.1073, 0.1138, 0.0506, 0.1145, 0.0423, 0.1136, 0.1145,\n",
            "         0.1145],\n",
            "        [0.1254, 0.1254, 0.0461, 0.1094, 0.0548, 0.1254, 0.0483, 0.1191, 0.1207,\n",
            "         0.1254],\n",
            "        [0.1330, 0.1333, 0.0490, 0.0539, 0.0490, 0.1333, 0.0493, 0.1332, 0.1327,\n",
            "         0.1333],\n",
            "        [0.1304, 0.1304, 0.0481, 0.0735, 0.0480, 0.1304, 0.0480, 0.1303, 0.1304,\n",
            "         0.1304],\n",
            "        [0.1155, 0.1155, 0.0512, 0.1087, 0.1082, 0.1155, 0.0425, 0.1118, 0.1155,\n",
            "         0.1155],\n",
            "        [0.1328, 0.1328, 0.0489, 0.1325, 0.0489, 0.1327, 0.0501, 0.0559, 0.1328,\n",
            "         0.1328],\n",
            "        [0.1543, 0.1543, 0.0567, 0.0569, 0.0572, 0.1543, 0.0568, 0.0951, 0.0602,\n",
            "         0.1543],\n",
            "        [0.1259, 0.1259, 0.1135, 0.0470, 0.0463, 0.1259, 0.0463, 0.1173, 0.1259,\n",
            "         0.1259],\n",
            "        [0.1264, 0.1264, 0.0487, 0.1260, 0.0465, 0.1264, 0.0465, 0.1264, 0.1005,\n",
            "         0.1264],\n",
            "        [0.1280, 0.1280, 0.0471, 0.0584, 0.0471, 0.1280, 0.0803, 0.1279, 0.1272,\n",
            "         0.1280],\n",
            "        [0.1234, 0.1234, 0.0455, 0.0696, 0.0647, 0.1234, 0.0825, 0.1210, 0.1232,\n",
            "         0.1234],\n",
            "        [0.1433, 0.1433, 0.0527, 0.0527, 0.0565, 0.1433, 0.0528, 0.1275, 0.0849,\n",
            "         0.1433],\n",
            "        [0.1173, 0.1173, 0.0432, 0.0774, 0.0883, 0.1173, 0.1171, 0.0876, 0.1173,\n",
            "         0.1173],\n",
            "        [0.1173, 0.1172, 0.0432, 0.1167, 0.0939, 0.1173, 0.0432, 0.1173, 0.1173,\n",
            "         0.1166],\n",
            "        [0.1183, 0.1173, 0.0436, 0.1184, 0.0859, 0.1182, 0.0436, 0.1177, 0.1185,\n",
            "         0.1185],\n",
            "        [0.1287, 0.1287, 0.0474, 0.0853, 0.0474, 0.1287, 0.0477, 0.1286, 0.1287,\n",
            "         0.1287],\n",
            "        [0.1199, 0.1200, 0.0441, 0.1199, 0.0758, 0.1198, 0.0441, 0.1164, 0.1200,\n",
            "         0.1200],\n",
            "        [0.1193, 0.1193, 0.0439, 0.0844, 0.1189, 0.1192, 0.0439, 0.1192, 0.1124,\n",
            "         0.1193],\n",
            "        [0.1333, 0.1340, 0.0493, 0.0495, 0.0493, 0.1340, 0.0518, 0.1316, 0.1330,\n",
            "         0.1340],\n",
            "        [0.1252, 0.1252, 0.0461, 0.1236, 0.0461, 0.1252, 0.0461, 0.1229, 0.1147,\n",
            "         0.1251]])\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-5sPEsgwLAb",
        "colab_type": "text"
      },
      "source": [
        "# Building Networks With PyTorch\n",
        "\n",
        "1. You need to subclass it from **nn.Module**\n",
        "2. You need to call **nn.Module**'s superclass\n",
        "\n",
        "```c\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "```\n",
        "\n",
        "3. Linear layer is the combination of weights and bias matrices. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSkHsy2NwfqX",
        "colab_type": "code",
        "outputId": "62fb6eb1-aa7d-4309-be06-c456f2a670e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class Network(nn.Module):\n",
        "  def __init__(self, hidden1_input_size, hidden1_output_size, hidden2_output_size):\n",
        "    super().__init__()\n",
        "    self.hidden = nn.Linear(hidden1_input_size, hidden1_output_size)\n",
        "    self.output = nn.Linear(hidden1_output_size, hidden2_output_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "  \n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.hidden(x)\n",
        "    x = self.sigmoid(x)\n",
        "    x = self.output(x)\n",
        "    x = self.softmax(x)\n",
        "    \n",
        "    return x\n",
        "   \n",
        "    \n",
        "model = Network(784, 256, 10)\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            "  (softmax): Softmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhZoKMKF2jfQ",
        "colab_type": "text"
      },
      "source": [
        "Another way to do this is to call <code>Sigmoid</code> from <code>import torch.nn.functional as F</code>. If you use it from nn, it creates objects and classes, but if you use it from functional, it won't create any objects and classes, because it simply does it functionally. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOsdzMyJ3QR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.hidden = nn.Linear(784, 256)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.output = nn.Linear(256, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Hidden layer with sigmoid activation\n",
        "        x = F.sigmoid(self.hidden(x))\n",
        "        # Output layer with softmax activation\n",
        "        x = F.softmax(self.output(x), dim=1)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdVGvV2_3VLu",
        "colab_type": "text"
      },
      "source": [
        "NOTE: It turns out that networks tend to train a lot faster using ReLu instead of tanh or sigmoid activation functions ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCZqjOQB5aWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import subprocess\n",
        "import numpy as np\n",
        "import os.path\n",
        "import pickle\n",
        "import scipy.io as sio\n",
        "\n",
        "# TODO:\n",
        "#ANNOTATION_FILE = '/home/suhyunkim011/Pasion/Diving.txt'\n",
        "ANNOTATION_FILE = 'Diving.txt'\n",
        "\n",
        "#MAT_FILE = '/home/suhyunkim011/Pasion/diving.mat'\n",
        "MAT_FILE = 'diving.mat'\n",
        "\n",
        "def get_pose_labels2():\n",
        "    # move the pictures to a certain directory and create labels\n",
        "    contents = sio.loadmat(MAT_FILE)\n",
        "    # print(len(contents))\n",
        "    tracked = contents['boxes_tracked_wholevideo']\n",
        "    # print(tracked)\n",
        "    # print(len(tracked))\n",
        "    # (298387, 107)\n",
        "    # print(tracked.shape)\n",
        "\n",
        "    # print(tracked[0].shape)\n",
        "\n",
        "    # batchsize = 298387; timesteps = 202; shape = 104;\n",
        "    arr_frame = []\n",
        "    arr_score = np.array([])\n",
        "    arr_score_flat = []\n",
        "    arr_difficulty = np.array([])\n",
        "    arr_difficulty_flat = []\n",
        "\n",
        "    with open(ANNOTATION_FILE) as filename:\n",
        "        max_counter = 202\n",
        "        group_counter = 0\n",
        "        one_group_counter = 0\n",
        "        max_one_group_counter = 202\n",
        "        supposed_to_be_counter = 0\n",
        "        for line in filename:\n",
        "            # print(line)\n",
        "            if '#' in line:\n",
        "                continue\n",
        "\n",
        "            if 'A' in line:\n",
        "\n",
        "                group_counter += 1\n",
        "                line_arr = line.split()\n",
        "                # print(line_arr[0])\n",
        "                # print(line_arr[1])\n",
        "                start = -1\n",
        "                end = -1\n",
        "\n",
        "                for i in range(0, 2):\n",
        "                    if i == 0:\n",
        "                        start = line_arr[0]\n",
        "                        start = int(start)\n",
        "\n",
        "                    elif i == 1:\n",
        "                        end = line_arr[1]\n",
        "                        end = int(end)\n",
        "\n",
        "                start = start - 1\n",
        "\n",
        "                pose_group = np.zeros([202, 104])\n",
        "\n",
        "                for i in range(start, end + 1):\n",
        "                    supposed_to_be_counter += 1\n",
        "                    converted = tracked[i][:104]\n",
        "                    pose_group[one_group_counter, :] = converted\n",
        "                    one_group_counter += 1\n",
        "\n",
        "                if one_group_counter > max_one_group_counter:\n",
        "                    max_one_group_counter = one_group_counter\n",
        "                    print(\"-------------*********-----------------WARNING-------------*********-----------------: %d\", one_group_counter)\n",
        "                # len(pose_group): 169; one_group_counter: 169; max_one_group_counter: 202\n",
        "\n",
        "                # print(pose_group)\n",
        "                arr_frame.append(pose_group)  # figure out why it worked when I put it above the if one_group_counter > max_one_group_counter:\n",
        "                one_group_counter = 0\n",
        "\n",
        "            if 'Score' in line:\n",
        "                line_arr = line.split()\n",
        "                arr_sub_total_score = np.empty(max_counter)\n",
        "                arr_sub_total_score.fill(line_arr[2])\n",
        "\n",
        "                arr_sub_difficulty_score = np.empty(max_counter)\n",
        "                arr_sub_difficulty_score.fill(line_arr[3])\n",
        "\n",
        "                arr_score = np.append(arr_score, arr_sub_total_score)\n",
        "                arr_difficulty = np.append(arr_difficulty, arr_sub_difficulty_score)\n",
        "                # print(arr_sub_total_score)\n",
        "\n",
        "                arr_score_flat.append(float(line_arr[2]))\n",
        "                arr_difficulty_flat.append(float(line_arr[3]))\n",
        "\n",
        "        # print(f'arr_frame: {len(arr_frame)}, arr_score_flat: {len(arr_score_flat)}, arr_difficulty_concat: {len(arr_difficulty_flat)}')\n",
        "        # print(f'arr_frame: {len(arr_frame)}')\n",
        "        # print(arr_frame[71])\n",
        "        print(np.array(arr_frame).shape)\n",
        "    return np.array(arr_frame), np.array(arr_score_flat), arr_difficulty_flat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFtVr8bU57aO",
        "colab_type": "code",
        "outputId": "b103c240-89e0-451c-b346-61168f339520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "arr_frames, scores, arr_difficulty = get_pose_labels2()\n",
        "print(scores.shape)\n",
        "images = torch.tensor(arr_frames)\n",
        "labels = torch.tensor(scores)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(159, 202, 104)\n",
            "(159,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MnhpStzYYYI",
        "colab_type": "code",
        "outputId": "0897b3e9-2a96-4f7e-8967-3551ad658707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "def correlation_coefficient_loss(x, y):\n",
        "  vx = x - torch.mean(x)\n",
        "  vy = y - torch.mean(y)\n",
        "\n",
        "  cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
        "  return cost\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 202\n",
        "input_size = 104\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "num_epochs = 2\n",
        "learning_rate = 0.01\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        h0 = torch.zeros(self.num_layers, input_size, self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, input_size, self.hidden_size)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = correlation_coefficient_loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(images)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, image in enumerate(images):\n",
        "        images = images.reshape(-1, sequence_length, input_size)\n",
        "     \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size)\n",
        "        labels = labels\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += len(labels)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-49b7de78116d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-49b7de78116d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, num_layers, num_classes)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m104\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mlayer_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mw_ih\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mw_hh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mb_ih\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkGRo8HtVTfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}