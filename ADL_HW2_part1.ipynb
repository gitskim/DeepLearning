{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADL_HW2_part1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "AXLQrGDowIIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "syNurw-Vwd-3",
        "colab_type": "code",
        "outputId": "6993d228-05ff-4b73-f057-7207b125d7cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 8\n",
        "GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192/*.tfrec'\n",
        "IMAGE_SIZE = [192, 192]\n",
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "filenames = tf.gfile.Glob(GCS_PATTERN)\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\n",
        "validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    features = {\n",
        "        \"image\": tf.FixedLenFeature((), tf.string), # tf.string means byte string\n",
        "        \"label\": tf.FixedLenFeature((), tf.string),\n",
        "        \"one_hot_label\": tf.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "    example = tf.parse_single_example(example, features)\n",
        "    image = tf.image.decode_jpeg(example['image'])\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    one_hot_label = tf.io.decode_raw(example['one_hot_label'], out_type=tf.uint8) # 'decode' byte string into byte list\n",
        "    one_hot_label = tf.cast(one_hot_label, tf.float32)  # convert one hot labels to floats\n",
        "    one_hot_label = tf.reshape(one_hot_label, [5])  # explicit fixed size needed on TPU\n",
        "    label = example['label']  # byte string\n",
        "    return image, label, one_hot_label\n",
        "  \n",
        "def load_dataset(filenames):  \n",
        "  # read from tfrecs\n",
        "  records = tf.data.TFRecordDataset(filenames, num_parallel_reads=32)  # this will read from multiple GCS files in parallel\n",
        "  dataset = records.map(read_tfrecord, num_parallel_calls=32)\n",
        "  return dataset\n",
        "\n",
        "def features_and_targets(image, label, one_hot_label):\n",
        "  feature = image\n",
        "  target = one_hot_label\n",
        "  return feature, target  # for training, a Keras model needs 2 items: features and targets\n",
        "\n",
        "def get_batched_dataset(filenames):\n",
        "  dataset = load_dataset(filenames)\n",
        "  dataset = dataset.map(features_and_targets, num_parallel_calls=32)\n",
        "  dataset = dataset.cache()  # This dataset fits in RAM\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # drop_remainder needed on TPU\n",
        "  dataset = dataset.prefetch(-1) # prefetch next batch while training  (-1: autotune prefetch buffer size)\n",
        "  # should shuffle too but this dataset was well shuffled on disk already\n",
        "  return dataset\n",
        "\n",
        "def get_training_dataset():\n",
        "  return get_batched_dataset(training_filenames)\n",
        "\n",
        "def get_validation_dataset():\n",
        "  return get_batched_dataset(validation_filenames)\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',include_top=False, input_shape=(192, 192, 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  loss= 'categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(get_training_dataset(), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                      validation_data=get_validation_dataset(), validation_steps=validation_steps)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pattern matches 16 data files. Splitting dataset into 13 training files and 3 validation files\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 5s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/8\n",
            "93/93 [==============================] - 40s 430ms/step - loss: 0.8366 - acc: 0.6939 - val_loss: 0.6061 - val_acc: 0.7857\n",
            "Epoch 2/8\n",
            "93/93 [==============================] - 25s 271ms/step - loss: 0.3856 - acc: 0.8716 - val_loss: 0.6015 - val_acc: 0.7798\n",
            "Epoch 3/8\n",
            "93/93 [==============================] - 25s 272ms/step - loss: 0.2282 - acc: 0.9335 - val_loss: 0.7057 - val_acc: 0.7634\n",
            "Epoch 4/8\n",
            "93/93 [==============================] - 25s 271ms/step - loss: 0.1781 - acc: 0.9442 - val_loss: 0.8777 - val_acc: 0.7455\n",
            "Epoch 5/8\n",
            "93/93 [==============================] - 25s 272ms/step - loss: 0.2210 - acc: 0.9177 - val_loss: 0.7471 - val_acc: 0.7693\n",
            "Epoch 6/8\n",
            "93/93 [==============================] - 25s 271ms/step - loss: 0.1605 - acc: 0.9402 - val_loss: 0.5444 - val_acc: 0.8274\n",
            "Epoch 7/8\n",
            "93/93 [==============================] - 25s 272ms/step - loss: 0.0885 - acc: 0.9731 - val_loss: 0.5550 - val_acc: 0.8363\n",
            "Epoch 8/8\n",
            "93/93 [==============================] - 25s 272ms/step - loss: 0.0356 - acc: 0.9963 - val_loss: 0.5936 - val_acc: 0.8229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nZ2rrBUPx9bP",
        "colab_type": "code",
        "outputId": "d1d5b521-0c1c-4e1f-e44c-92d44aeffbf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 8\n",
        "GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192/*.tfrec'\n",
        "IMAGE_SIZE = [192, 192]\n",
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "filenames = tf.gfile.Glob(GCS_PATTERN)\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\n",
        "validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',include_top=False, input_shape=(192, 192, 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  loss= 'categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(get_training_dataset(), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                      validation_data=get_validation_dataset(), validation_steps=validation_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pattern matches 16 data files. Splitting dataset into 13 training files and 3 validation files\n",
            "Epoch 1/8\n",
            "93/93 [==============================] - 29s 312ms/step - loss: 0.8949 - acc: 0.6650 - val_loss: 0.6044 - val_acc: 0.7887\n",
            "Epoch 2/8\n",
            "93/93 [==============================] - 25s 274ms/step - loss: 0.4172 - acc: 0.8555 - val_loss: 0.6085 - val_acc: 0.7812\n",
            "Epoch 3/8\n",
            "93/93 [==============================] - 25s 272ms/step - loss: 0.2793 - acc: 0.9113 - val_loss: 0.8805 - val_acc: 0.6994\n",
            "Epoch 4/8\n",
            "93/93 [==============================] - 25s 273ms/step - loss: 0.2589 - acc: 0.9073 - val_loss: 0.6945 - val_acc: 0.7723\n",
            "Epoch 5/8\n",
            "93/93 [==============================] - 25s 273ms/step - loss: 0.1815 - acc: 0.9395 - val_loss: 0.5181 - val_acc: 0.8318\n",
            "Epoch 6/8\n",
            "93/93 [==============================] - 25s 273ms/step - loss: 0.0911 - acc: 0.9724 - val_loss: 0.5432 - val_acc: 0.8289\n",
            "Epoch 7/8\n",
            "93/93 [==============================] - 25s 273ms/step - loss: 0.0490 - acc: 0.9913 - val_loss: 0.5538 - val_acc: 0.8214\n",
            "Epoch 8/8\n",
            "93/93 [==============================] - 25s 273ms/step - loss: 0.0294 - acc: 0.9987 - val_loss: 0.5445 - val_acc: 0.8304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KFWUHgx43lKq",
        "colab_type": "code",
        "outputId": "6d6f8cb1-e626-4ca6-a5db-7b9c1a437f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 8\n",
        "GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192/*.tfrec'\n",
        "IMAGE_SIZE = [192, 192]\n",
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "filenames = tf.gfile.Glob(GCS_PATTERN)\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\n",
        "validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "\n",
        "\n",
        "# conv_base = VGG16(weights='imagenet',include_top=False, input_shape=(192, 192, 3))\n",
        "conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(192, 192, 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  loss= 'categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(get_training_dataset(), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                      validation_data=get_validation_dataset(), validation_steps=validation_steps)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pattern matches 16 data files. Splitting dataset into 13 training files and 3 validation files\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 9s 0us/step\n",
            "Epoch 1/8\n",
            "93/93 [==============================] - 32s 342ms/step - loss: 7.5292 - acc: 0.5128 - val_loss: 12.2565 - val_acc: 0.2396\n",
            "Epoch 2/8\n",
            "93/93 [==============================] - 23s 244ms/step - loss: 7.1831 - acc: 0.5484 - val_loss: 12.2593 - val_acc: 0.2366\n",
            "Epoch 3/8\n",
            "93/93 [==============================] - 23s 244ms/step - loss: 6.1310 - acc: 0.6095 - val_loss: 12.3044 - val_acc: 0.2366\n",
            "Epoch 4/8\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 5.3413 - acc: 0.6610 - val_loss: 12.2805 - val_acc: 0.2381\n",
            "Epoch 5/8\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 5.0811 - acc: 0.6798 - val_loss: 11.5826 - val_acc: 0.2723\n",
            "Epoch 6/8\n",
            "93/93 [==============================] - 23s 246ms/step - loss: 4.7639 - acc: 0.6989 - val_loss: 9.2631 - val_acc: 0.4122\n",
            "Epoch 7/8\n",
            "93/93 [==============================] - 23s 246ms/step - loss: 3.6250 - acc: 0.7658 - val_loss: 7.6745 - val_acc: 0.5134\n",
            "Epoch 8/8\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 2.7646 - acc: 0.8209 - val_loss: 6.1846 - val_acc: 0.6042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pOdyjuZT5zm1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}