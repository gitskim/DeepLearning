{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADL_HW2_part1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "AXLQrGDowIIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, MobileNet\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wV-z0T71JHPw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Download the ​mini-flowers​ dataset. This contains 1,500 images of five different types of flowers. Modify one of the above notebooks to classify these images using transfer learning. How accurate of a model can you train?\n",
        "\n",
        "I tried transfer learning from VGG16. I tried two different models: one with 128-node dense layer at the end and the other one with 64 layers. They both have a very similar accuracy of less than 85% accuracy. I also tried other variants of layers, which are not recorded in this notebook, (I only my highest accuracy results), but the highest accruacy I could get was 83%. \n",
        "\n",
        "Sources: https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_solution.ipynb#scrollTo=wX9ISOABudWh"
      ]
    },
    {
      "metadata": {
        "id": "syNurw-Vwd-3",
        "colab_type": "code",
        "outputId": "cb6f3a11-a976-413b-e8a5-b7c0d528b63d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 8\n",
        "GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192/*.tfrec'\n",
        "IMAGE_SIZE = [192, 192]\n",
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "filenames = tf.gfile.Glob(GCS_PATTERN)\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\n",
        "validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    features = {\n",
        "        \"image\": tf.FixedLenFeature((), tf.string), # tf.string means byte string\n",
        "        \"label\": tf.FixedLenFeature((), tf.string),\n",
        "        \"one_hot_label\": tf.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "    example = tf.parse_single_example(example, features)\n",
        "    image = tf.image.decode_jpeg(example['image'])\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    one_hot_label = tf.io.decode_raw(example['one_hot_label'], out_type=tf.uint8) # 'decode' byte string into byte list\n",
        "    one_hot_label = tf.cast(one_hot_label, tf.float32)  # convert one hot labels to floats\n",
        "    one_hot_label = tf.reshape(one_hot_label, [5])  # explicit fixed size needed on TPU\n",
        "    label = example['label']  # byte string\n",
        "    return image, label, one_hot_label\n",
        "  \n",
        "def load_dataset(filenames):  \n",
        "  # read from tfrecs\n",
        "  records = tf.data.TFRecordDataset(filenames, num_parallel_reads=32)  # this will read from multiple GCS files in parallel\n",
        "  dataset = records.map(read_tfrecord, num_parallel_calls=32)\n",
        "  return dataset\n",
        "\n",
        "def features_and_targets(image, label, one_hot_label):\n",
        "  feature = image\n",
        "  target = one_hot_label\n",
        "  return feature, target  # for training, a Keras model needs 2 items: features and targets\n",
        "\n",
        "def get_batched_dataset(filenames):\n",
        "  dataset = load_dataset(filenames)\n",
        "  dataset = dataset.map(features_and_targets, num_parallel_calls=32)\n",
        "  dataset = dataset.cache()  # This dataset fits in RAM\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # drop_remainder needed on TPU\n",
        "  dataset = dataset.prefetch(-1) # prefetch next batch while training  (-1: autotune prefetch buffer size)\n",
        "  # should shuffle too but this dataset was well shuffled on disk already\n",
        "  return dataset\n",
        "\n",
        "def get_training_dataset():\n",
        "  return get_batched_dataset(training_filenames)\n",
        "\n",
        "def get_validation_dataset():\n",
        "  return get_batched_dataset(validation_filenames)\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',include_top=False, input_shape=(192, 192, 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  loss= 'categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(get_training_dataset(), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                      validation_data=get_validation_dataset(), validation_steps=validation_steps)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pattern matches 16 data files. Splitting dataset into 13 training files and 3 validation files\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/8\n",
            "93/93 [==============================] - 34s 370ms/step - loss: 1.0068 - acc: 0.6522 - val_loss: 0.6078 - val_acc: 0.7842\n",
            "Epoch 2/8\n",
            "93/93 [==============================] - 26s 275ms/step - loss: 0.4317 - acc: 0.8558 - val_loss: 0.6105 - val_acc: 0.7783\n",
            "Epoch 3/8\n",
            "93/93 [==============================] - 26s 274ms/step - loss: 0.2768 - acc: 0.9210 - val_loss: 0.5351 - val_acc: 0.8006\n",
            "Epoch 4/8\n",
            "93/93 [==============================] - 25s 273ms/step - loss: 0.1915 - acc: 0.9499 - val_loss: 0.5109 - val_acc: 0.7991\n",
            "Epoch 5/8\n",
            "93/93 [==============================] - 26s 275ms/step - loss: 0.1179 - acc: 0.9772 - val_loss: 0.5043 - val_acc: 0.8274\n",
            "Epoch 6/8\n",
            "93/93 [==============================] - 25s 274ms/step - loss: 0.0800 - acc: 0.9899 - val_loss: 0.4876 - val_acc: 0.8289\n",
            "Epoch 7/8\n",
            "93/93 [==============================] - 26s 275ms/step - loss: 0.0515 - acc: 0.9950 - val_loss: 0.5805 - val_acc: 0.8036\n",
            "Epoch 8/8\n",
            "93/93 [==============================] - 25s 274ms/step - loss: 0.0347 - acc: 0.9987 - val_loss: 0.6658 - val_acc: 0.7827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JdWGf5syMTXn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Next, run experiments using at least two pretrained convolutional bases (​applications​), and compare your results. Include a short, informal write-up (using bullet points is fine). What differences do you see, and why? Read the associated papers to learn more about the networks you’re using, linked from the API doc.\n",
        "\n",
        "I have tried VGG16, ResNet, and MobileNet. VGG16.\n",
        "\n",
        "VGG16: 80% accuracy\n",
        "\n",
        "ResNet: 52% accruacy\n",
        "\n",
        "MobileNet: 80% accuracy\n",
        "\n",
        "Out of all of those models, MobileNet was fastest. I belive that's because the model is built to utilize limited resources with fast performance. \n",
        "\n",
        "VGG16 and MobileNet's accuracy came out to be the same and ResNet's accuracy was the loewst. I believe the reason ResNet's accruacy came out so low is because its dataset that the model's trained on is not as similar with flowers datasets as VGG16 or that of MobileNet. \n"
      ]
    },
    {
      "metadata": {
        "id": "nZ2rrBUPx9bP",
        "colab_type": "code",
        "outputId": "3a5eb323-0e1b-4ebc-ae90-63f27d1619c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 8\n",
        "GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192/*.tfrec'\n",
        "IMAGE_SIZE = [192, 192]\n",
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "filenames = tf.gfile.Glob(GCS_PATTERN)\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\n",
        "validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',include_top=False, input_shape=(192, 192, 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  loss= 'categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(get_training_dataset(), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                      validation_data=get_validation_dataset(), validation_steps=validation_steps)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pattern matches 16 data files. Splitting dataset into 13 training files and 3 validation files\n",
            "Epoch 1/8\n",
            "93/93 [==============================] - 29s 309ms/step - loss: 0.9188 - acc: 0.6996 - val_loss: 0.7746 - val_acc: 0.7560\n",
            "Epoch 2/8\n",
            "93/93 [==============================] - 25s 274ms/step - loss: 0.3724 - acc: 0.8730 - val_loss: 0.5839 - val_acc: 0.7932\n",
            "Epoch 3/8\n",
            "93/93 [==============================] - 26s 275ms/step - loss: 0.2305 - acc: 0.9220 - val_loss: 0.9141 - val_acc: 0.7277\n",
            "Epoch 4/8\n",
            "93/93 [==============================] - 25s 274ms/step - loss: 0.1818 - acc: 0.9395 - val_loss: 0.6581 - val_acc: 0.7872\n",
            "Epoch 5/8\n",
            "93/93 [==============================] - 26s 276ms/step - loss: 0.1698 - acc: 0.9365 - val_loss: 0.6440 - val_acc: 0.7857\n",
            "Epoch 6/8\n",
            "93/93 [==============================] - 26s 276ms/step - loss: 0.0831 - acc: 0.9735 - val_loss: 0.5355 - val_acc: 0.8304\n",
            "Epoch 7/8\n",
            "93/93 [==============================] - 26s 276ms/step - loss: 0.0497 - acc: 0.9886 - val_loss: 0.5730 - val_acc: 0.8110\n",
            "Epoch 8/8\n",
            "93/93 [==============================] - 26s 275ms/step - loss: 0.0221 - acc: 0.9980 - val_loss: 0.6539 - val_acc: 0.8051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KFWUHgx43lKq",
        "colab_type": "code",
        "outputId": "5a5a6867-d522-43d8-c930-b513d9a6acca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 8\n",
        "GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192/*.tfrec'\n",
        "IMAGE_SIZE = [192, 192]\n",
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "filenames = tf.gfile.Glob(GCS_PATTERN)\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\n",
        "validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "\n",
        "\n",
        "conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(192, 192, 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  loss= 'categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(get_training_dataset(), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                      validation_data=get_validation_dataset(), validation_steps=validation_steps)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pattern matches 16 data files. Splitting dataset into 13 training files and 3 validation files\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 1s 0us/step\n",
            "Epoch 1/8\n",
            "93/93 [==============================] - 28s 306ms/step - loss: 5.9984 - acc: 0.5998 - val_loss: 10.5950 - val_acc: 0.2396\n",
            "Epoch 2/8\n",
            "93/93 [==============================] - 23s 249ms/step - loss: 5.2276 - acc: 0.6623 - val_loss: 11.2420 - val_acc: 0.1905\n",
            "Epoch 3/8\n",
            "93/93 [==============================] - 23s 248ms/step - loss: 4.8178 - acc: 0.6922 - val_loss: 12.2829 - val_acc: 0.2366\n",
            "Epoch 4/8\n",
            "93/93 [==============================] - 23s 248ms/step - loss: 4.4570 - acc: 0.7174 - val_loss: 11.4012 - val_acc: 0.2426\n",
            "Epoch 5/8\n",
            "93/93 [==============================] - 23s 249ms/step - loss: 4.1370 - acc: 0.7372 - val_loss: 9.9512 - val_acc: 0.3631\n",
            "Epoch 6/8\n",
            "93/93 [==============================] - 23s 249ms/step - loss: 4.2906 - acc: 0.7285 - val_loss: 9.0679 - val_acc: 0.4182\n",
            "Epoch 7/8\n",
            "93/93 [==============================] - 23s 248ms/step - loss: 4.2094 - acc: 0.7335 - val_loss: 7.4845 - val_acc: 0.5208\n",
            "Epoch 8/8\n",
            "93/93 [==============================] - 23s 244ms/step - loss: 4.1182 - acc: 0.7413 - val_loss: 5.7965 - val_acc: 0.6295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l63LaUliNjUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "8ff04706-0c70-4cdf-b65e-5f870408cf8b"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 8\n",
        "GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192/*.tfrec'\n",
        "IMAGE_SIZE = [192, 192]\n",
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "filenames = tf.gfile.Glob(GCS_PATTERN)\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\n",
        "validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "\n",
        "\n",
        "conv_base = MobileNet(weights='imagenet', include_top=False, input_shape=(192, 192, 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  loss= 'categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(get_training_dataset(), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                      validation_data=get_validation_dataset(), validation_steps=validation_steps)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pattern matches 16 data files. Splitting dataset into 13 training files and 3 validation files\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_192_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "Epoch 1/8\n",
            "93/93 [==============================] - 12s 130ms/step - loss: 7.3270 - acc: 0.5198 - val_loss: 6.7170 - val_acc: 0.5759\n",
            "Epoch 2/8\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 5.6711 - acc: 0.6361 - val_loss: 4.5909 - val_acc: 0.7039\n",
            "Epoch 3/8\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 4.4345 - acc: 0.7127 - val_loss: 4.6243 - val_acc: 0.7039\n",
            "Epoch 4/8\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 3.8218 - acc: 0.7483 - val_loss: 3.1555 - val_acc: 0.7902\n",
            "Epoch 5/8\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 2.2782 - acc: 0.8471 - val_loss: 2.7400 - val_acc: 0.8185\n",
            "Epoch 6/8\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 2.3884 - acc: 0.8401 - val_loss: 2.5092 - val_acc: 0.8289\n",
            "Epoch 7/8\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 1.9158 - acc: 0.8737 - val_loss: 3.1883 - val_acc: 0.7917\n",
            "Epoch 8/8\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 1.8751 - acc: 0.8763 - val_loss: 2.9722 - val_acc: 0.8065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qQmny3L4Njod",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}