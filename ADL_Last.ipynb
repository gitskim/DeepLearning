{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADL_Last.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "r6BJlRS8RCGj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "c6c1559c-2985-4548-f642-220a56b325d0"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 332.1MB 56kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 6.4MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 30.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 419kB 4.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A1IWrAzORMaN",
        "colab_type": "code",
        "outputId": "8734710b-7f8d-49cb-9be4-c770cd9750f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu # https://github.com/mjpost/sacreBLEU"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/dc/3ea54419418beb7d0ee0dedd08938519980b951419983a3ddd573c357190/sacrebleu-1.3.2.tar.gz\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Building wheels for collected packages: sacrebleu\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/f1/a9/4cce0ec602e8d195da27bb9b8f6708ec778fbafdbabb097fde\n",
            "Successfully built sacrebleu\n",
            "Installing collected packages: sacrebleu\n",
            "Successfully installed sacrebleu-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "irn4h_mF833N",
        "colab_type": "code",
        "outputId": "68eebfce-e01a-42fc-f8f6-4de6bf925e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-25 22:01:25--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.234.128, 2607:f8b0:4001:c1b::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.234.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-04-25 22:01:25 (166 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qkeWkFn9DBNp",
        "colab_type": "code",
        "outputId": "06a63e94-34f6-44c5-cd69-64505753f578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "zipfile1 = '/content/spa-eng.zip'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XXlAQzb9DjFc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(zipfile1, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uy1_ldwE7pPc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ]
    },
    {
      "metadata": {
        "id": "mwkTNW_8D72J",
        "colab_type": "code",
        "outputId": "42f3fac9-debd-4a35-c712-6018f53d07b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# !ls ./spa-eng/\n",
        "# !cat ./spa-eng/spa.txt\n",
        "\n",
        "sentences = []\n",
        "counter = 0\n",
        "with open(\"./spa-eng/spa.txt\",'r') as f:\n",
        "    for line in f:\n",
        "        counter += 1\n",
        "        if counter == 4001:\n",
        "          break\n",
        "        line = line.split('\\t')\n",
        "        sentences.append((line[0],line[1]))\n",
        "\n",
        "\n",
        "print(len(sentences))\n",
        "npsen = np.array(sentences)\n",
        "print(npsen.shape)\n",
        "# print(sentences)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000\n",
            "(4000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1h_At00sRZcL",
        "colab_type": "code",
        "outputId": "a65b2c43-7356-44e6-ca66-5aaaa5a9c178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import sacrebleu\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import unicodedata\n",
        "\n",
        "# sentences = [\n",
        "#   (\"Do you want a cup of coffee?\", \"¿Quieres una taza de café?\"),\n",
        "#   (\"I've had coffee already.\", \"Ya tomé café.\"),\n",
        "#   (\"Can I get you a coffee?\", \"¿Quieres que te traiga un café?\"),\n",
        "#   (\"Please give me some coffee.\", \"Dame algo de café por favor.\"),\n",
        "#   (\"Would you like me to make coffee?\", \"¿Quieres que prepare café?\"),\n",
        "#   (\"Two coffees, please.\", \"Dos cafés, por favor.\"),\n",
        "#   (\"How about a cup of coffee?\", \"¿Qué tal una taza de café?\"),\n",
        "#   (\"I drank two cups of coffee.\", \"Me tomé dos tazas de café.\"),\n",
        "#   (\"Would you like to have a cup of coffee?\", \"¿Te gustaría tomar una taza de café?\"),\n",
        "#   (\"There'll be coffee and cake at five.\", \"A las cinco habrá café y un pastel.\"),\n",
        "#   (\"Another coffee, please.\", \"Otro café, por favor.\"),\n",
        "#   (\"I made coffee.\", \"Hice café.\"),\n",
        "#   (\"I would like to have a cup of coffee.\", \"Quiero beber una taza de café.\"),\n",
        "#   (\"Do you want me to make coffee?\", \"¿Quieres que haga café?\"),\n",
        "#   (\"It is hard to wake up without a strong cup of coffee.\", \"Es difícil despertarse sin una taza de café fuerte.\"),\n",
        "#   (\"All I drank was coffee.\", \"Todo lo que bebí fue café.\"),\n",
        "#   (\"I've drunk way too much coffee today.\", \"He bebido demasiado café hoy.\"),\n",
        "#   (\"Which do you prefer, tea or coffee?\", \"¿Qué prefieres, té o café?\"),\n",
        "#   (\"There are many kinds of coffee.\", \"Hay muchas variedades de café.\"),\n",
        "#   (\"I will make some coffee.\",\t\"Prepararé algo de café.\")\n",
        "# ]\n",
        "npsen = np.array(sentences)\n",
        "print(npsen.shape)\n",
        "def preprocess(s):\n",
        "  # for details, see https://www.tensorflow.org/alpha/tutorials/sequences/nmt_with_attention\n",
        "  s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "  s = s.strip()\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s\n",
        "\n",
        "\n",
        "print(\"Original:\", sentences[0])\n",
        "sentences = [(preprocess(source), preprocess(target)) for (source, target) in sentences]\n",
        "print(\"Preprocessed:\", sentences[0])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000, 2)\n",
            "Original: ('Go.', 'Ve.\\n')\n",
            "Preprocessed: ('<start> Go . <end>', '<start> Ve . <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Se4X7FsgRhuL",
        "colab_type": "code",
        "outputId": "d00eb96f-29f6-403e-9b69-c351860a8496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2876
        }
      },
      "cell_type": "code",
      "source": [
        "source_sentences, target_sentences = list(zip(*sentences))\n",
        "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "source_tokenizer.fit_on_texts(source_sentences)\n",
        "source_data = source_tokenizer.texts_to_sequences(source_sentences)\n",
        "print(\"Sequence:\", source_data[0])\n",
        "source_data = tf.keras.preprocessing.sequence.pad_sequences(source_data, padding='post')\n",
        "print(\"Padded:\", source_data[0])\n",
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "target_tokenizer.fit_on_texts(target_sentences)\n",
        "target_data = target_tokenizer.texts_to_sequences(target_sentences)\n",
        "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post')\n",
        "\n",
        "# Create labels for the decoder by shifting the target sequence\n",
        "# one to the right.\n",
        "target_labels = np.zeros(target_data.shape)\n",
        "target_labels[:,0:target_data.shape[1] -1] = target_data[:,1:]\n",
        "\n",
        "print(\"Target sequence\", target_data[0])\n",
        "print(\"Target label\", target_labels[0])\n",
        "\n",
        "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "def decode(encoded, tokenizer):\n",
        "  for number in encoded:\n",
        "    if number !=0:\n",
        "      print (\"%d -> %s\" % (number, tokenizer.index_word[number]))\n",
        "      \n",
        "decode(source_data[0], source_tokenizer)\n",
        "\n",
        "batch_size = 5\n",
        "dataset = tf.data.Dataset.from_tensor_slices((source_data, target_data, target_labels)).batch(batch_size)\n",
        "\n",
        "example_batch = next(iter(dataset))\n",
        "source, target, taget_labels = example_batch\n",
        "print(\"Shapes:\", source.shape, target.shape, taget_labels.shape)\n",
        "\n",
        "embedding_size = 32\n",
        "rnn_size = 64\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(source_vocab_size,\n",
        "                                               embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)        \n",
        "    return output, state\n",
        "  \n",
        "  def init_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, rnn_size))\n",
        "  \n",
        "  \n",
        "# Create a batch of one sentence\n",
        "ex_sentence = tf.expand_dims(source_data[0], axis=0)\n",
        "ex_translation = tf.expand_dims(target_data[0], axis=0)\n",
        "ex_labels = tf.expand_dims(target_labels[0], axis=0)\n",
        "print(ex_sentence.shape)\n",
        "\n",
        "encoder = Encoder()\n",
        "hidden_state = encoder.init_state(batch_size=1)\n",
        "print(hidden_state.shape)\n",
        "\n",
        "output, hidden_state = encoder(ex_sentence, hidden_state)\n",
        "print(output.shape)\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, \n",
        "                                               embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    logits = self.dense(output)\n",
        "    return logits, state\n",
        "  \n",
        "\n",
        "decoder = Decoder()\n",
        "decoder_output, decoder_state = decoder(ex_labels, hidden_state)\n",
        "print(decoder_output.shape)\n",
        "\n",
        "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def calc_loss(targets, logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  return crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "print(\"Loss\", calc_loss(ex_labels, decoder_output))\n",
        "\n",
        "\n",
        "def translate(idx=None):\n",
        "  \n",
        "    if idx == None: \n",
        "      idx = np.random.choice(len(sentences))\n",
        "    \n",
        "    input_sent = source_data[idx]\n",
        "    input_sent = tf.expand_dims(input_sent, axis=0)\n",
        "    \n",
        "    hidden_state = encoder.init_state(batch_size=1)\n",
        "    output, hidden_state = encoder(input_sent, hidden_state)\n",
        "    \n",
        "    decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "    out_words = []\n",
        "    \n",
        "    decoder_state = hidden_state\n",
        "\n",
        "    while True:\n",
        "      \n",
        "        decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
        "        decoder_input = tf.argmax(decoder_output, -1)\n",
        "        word_idx = decoder_input.numpy()[0][0]\n",
        "        # if we've predicted 0 (which is reserved, usually this will only happen\n",
        "        # before the decoder is trained, just stop translating and return\n",
        "        # what we have)\n",
        "        if word_idx == 0: \n",
        "          out_words.append('<end>')\n",
        "        else:\n",
        "          out_words.append(target_tokenizer.index_word[word_idx])\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "          break\n",
        "          \n",
        "    translation = ' '.join(out_words)    \n",
        "    return sentences[idx][0], sentences[idx][1], translation\n",
        "  \n",
        "input_sent, target_sent, translation = translate()\n",
        "print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "@tf.function # remove this annotation when debugging\n",
        "def train_step(source_seq, target_seq, target_labels, initial_state):\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    encoder_output, encoder_state = encoder(source_seq, initial_state)\n",
        "    logits, decoder_state = decoder(target_seq, encoder_state)\n",
        "    loss = calc_loss(target_labels, logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss\n",
        "\n",
        "EPOCHS = 300\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "  \n",
        "    en_initial_states = encoder.init_state(batch_size)\n",
        "    \n",
        "    for batch, (source_seq, target_seq, target_labels) in enumerate(dataset):\n",
        "      loss = train_step(source_seq, target_seq, target_labels, en_initial_states)\n",
        "      elapsed = time.time() - start\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      print(\"Epoch #%d, Loss %.4f, Time %.2f sec\" % (epoch, loss, elapsed))\n",
        "      input_sent, target_sent, translation = translate()\n",
        "      print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))\n",
        "      \n",
        "      \n",
        "references, hypotheses = [], []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  input_sent, target_sent, translation = translate()\n",
        "  references.append(target_sent)\n",
        "  hypotheses.append(\"<start> \" + translation)\n",
        "  \n",
        "results = sacrebleu.raw_corpus_bleu(hypotheses, [references])\n",
        "print(results)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence: [1, 18, 3, 2]\n",
            "Padded: [ 1 18  3  2  0  0  0  0]\n",
            "Target sequence [ 1 62  3  2  0  0  0  0  0  0  0  0]\n",
            "Target label [62.  3.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            "1 -> <start>\n",
            "18 -> go\n",
            "3 -> .\n",
            "2 -> <end>\n",
            "Shapes: (5, 8) (5, 12) (5, 12)\n",
            "(1, 8)\n",
            "(1, 64)\n",
            "(1, 8, 64)\n",
            "(1, 12, 2376)\n",
            "Loss tf.Tensor(1.9435749, shape=(), dtype=float32)\n",
            "Input: <start> Have a seat . <end>\n",
            "Target: <start> Tome asiento . <end>\n",
            "Translation: brazo teje encendida participar consigueme lunes hazmelo apagalo magico tenerlo bebio caliente miralo conozco cambiamos centrarte escribe ataque ayudanos abri\n",
            "\n",
            "Epoch #0, Loss 1.8491, Time 7.20 sec\n",
            "Input: <start> Tom lost . <end>\n",
            "Target: <start> Tom perdio . <end>\n",
            "Translation: ¿ ¿ . <end>\n",
            "\n",
            "Epoch #10, Loss 0.8980, Time 5.67 sec\n",
            "Input: <start> Look at this . <end>\n",
            "Target: <start> Fijate en esto . <end>\n",
            "Translation: deja la segundo . <end>\n",
            "\n",
            "Epoch #20, Loss 0.6048, Time 4.95 sec\n",
            "Input: <start> She s pretty . <end>\n",
            "Target: <start> Es guapa . <end>\n",
            "Translation: es tranquila . <end>\n",
            "\n",
            "Epoch #30, Loss 0.4072, Time 4.97 sec\n",
            "Input: <start> I m mean . <end>\n",
            "Target: <start> Soy malo . <end>\n",
            "Translation: estoy muy gorda . <end>\n",
            "\n",
            "Epoch #40, Loss 0.2832, Time 5.08 sec\n",
            "Input: <start> Don t yell . <end>\n",
            "Target: <start> No grites . <end>\n",
            "Translation: no te rindas . <end>\n",
            "\n",
            "Epoch #50, Loss 0.1740, Time 5.05 sec\n",
            "Input: <start> He tries . <end>\n",
            "Target: <start> El lo intenta . <end>\n",
            "Translation: el la beso . <end>\n",
            "\n",
            "Epoch #60, Loss 0.0908, Time 5.04 sec\n",
            "Input: <start> Tom failed . <end>\n",
            "Target: <start> Tom reprobo . <end>\n",
            "Translation: tom grito . <end>\n",
            "\n",
            "Epoch #70, Loss 0.0427, Time 5.06 sec\n",
            "Input: <start> Tom looked . <end>\n",
            "Target: <start> Tom miro . <end>\n",
            "Translation: tom insistio . <end>\n",
            "\n",
            "Epoch #80, Loss 0.0432, Time 5.08 sec\n",
            "Input: <start> Tom s left . <end>\n",
            "Target: <start> Tom se ha marchado . <end>\n",
            "Translation: tom se ha ido . <end>\n",
            "\n",
            "Epoch #90, Loss 0.0278, Time 5.01 sec\n",
            "Input: <start> Can you swim ? <end>\n",
            "Target: <start> ¿ Sabes nadar ? <end>\n",
            "Translation: ¿ podemos marcharnos ? <end>\n",
            "\n",
            "Epoch #100, Loss 0.0193, Time 5.00 sec\n",
            "Input: <start> I m drowning . <end>\n",
            "Target: <start> Me estoy ahogando . <end>\n",
            "Translation: me estoy ahogando . <end>\n",
            "\n",
            "Epoch #110, Loss 0.0080, Time 5.07 sec\n",
            "Input: <start> Who won ? <end>\n",
            "Target: <start> ¿ Quien ha ganado ? <end>\n",
            "Translation: ¿ quien va a pagar ? <end>\n",
            "\n",
            "Epoch #120, Loss 0.0065, Time 5.10 sec\n",
            "Input: <start> Time flies . <end>\n",
            "Target: <start> El tiempo vuela . <end>\n",
            "Translation: el tiempo vuela . <end>\n",
            "\n",
            "Epoch #130, Loss 0.0046, Time 5.01 sec\n",
            "Input: <start> Look at it . <end>\n",
            "Target: <start> Observalo . <end>\n",
            "Translation: miralo . <end>\n",
            "\n",
            "Epoch #140, Loss 0.0100, Time 5.01 sec\n",
            "Input: <start> Are you tired ? <end>\n",
            "Target: <start> ¿ Estas cansado ? <end>\n",
            "Translation: ¿ estas cansada ? <end>\n",
            "\n",
            "Epoch #150, Loss 0.0038, Time 5.69 sec\n",
            "Input: <start> Describe Tom . <end>\n",
            "Target: <start> Describilo a Tomas . <end>\n",
            "Translation: describelo a tomas . <end>\n",
            "\n",
            "Epoch #160, Loss 0.0031, Time 5.00 sec\n",
            "Input: <start> Wake up ! <end>\n",
            "Target: <start> Despiertate ! <end>\n",
            "Translation: despertate ! <end>\n",
            "\n",
            "Epoch #170, Loss 0.0026, Time 5.00 sec\n",
            "Input: <start> Tom drove . <end>\n",
            "Target: <start> Tomas condujo . <end>\n",
            "Translation: tomas condujo . <end>\n",
            "\n",
            "Epoch #180, Loss 0.0026, Time 5.84 sec\n",
            "Input: <start> He s a DJ . <end>\n",
            "Target: <start> El es DJ . <end>\n",
            "Translation: el es dj . <end>\n",
            "\n",
            "Epoch #190, Loss 0.0062, Time 4.98 sec\n",
            "Input: <start> Go . <end>\n",
            "Target: <start> Ve . <end>\n",
            "Translation: vete . <end>\n",
            "\n",
            "Epoch #200, Loss 0.0012, Time 5.04 sec\n",
            "Input: <start> Who ll go ? <end>\n",
            "Target: <start> ¿ Quien ira ? <end>\n",
            "Translation: ¿ quien ira ? <end>\n",
            "\n",
            "Epoch #210, Loss 0.0021, Time 5.04 sec\n",
            "Input: <start> How romantic ! <end>\n",
            "Target: <start> Que romantico ! <end>\n",
            "Translation: que romantico ! <end>\n",
            "\n",
            "Epoch #220, Loss 0.0168, Time 4.90 sec\n",
            "Input: <start> Call the cops . <end>\n",
            "Target: <start> Llama a la Policia . <end>\n",
            "Translation: llama a la policia . <end>\n",
            "\n",
            "Epoch #230, Loss 0.0020, Time 5.01 sec\n",
            "Input: <start> Call security ! <end>\n",
            "Target: <start> Llamad a seguridad ! <end>\n",
            "Translation: llamen a seguridad ! <end>\n",
            "\n",
            "Epoch #240, Loss 0.0065, Time 5.02 sec\n",
            "Input: <start> Can I try it ? <end>\n",
            "Target: <start> ¿ Puedo intentar ? <end>\n",
            "Translation: ¿ puedo intentar ? <end>\n",
            "\n",
            "Epoch #250, Loss 0.0010, Time 5.02 sec\n",
            "Input: <start> Keep quiet ! <end>\n",
            "Target: <start> Quedate tranquilo ! <end>\n",
            "Translation: quedate tranquilo ! <end>\n",
            "\n",
            "Epoch #260, Loss 0.0019, Time 5.02 sec\n",
            "Input: <start> We re sleepy . <end>\n",
            "Target: <start> Tenemos sueno . <end>\n",
            "Translation: tenemos sueno . <end>\n",
            "\n",
            "Epoch #270, Loss 0.0007, Time 5.02 sec\n",
            "Input: <start> We can meet . <end>\n",
            "Target: <start> Podemos encontrarnos . <end>\n",
            "Translation: podemos vernos . <end>\n",
            "\n",
            "Epoch #280, Loss 0.0031, Time 5.08 sec\n",
            "Input: <start> I m autistic . <end>\n",
            "Target: <start> Soy autista . <end>\n",
            "Translation: soy autista . <end>\n",
            "\n",
            "Epoch #290, Loss 0.0016, Time 5.22 sec\n",
            "Input: <start> We know you . <end>\n",
            "Target: <start> Le conocemos a usted . <end>\n",
            "Translation: a ti te conocemos . <end>\n",
            "\n",
            "BLEU(score=36.10502941451068, counts=[17145, 8748, 4290, 1644], totals=[22432, 18432, 14432, 10432], precisions=[76.43099144079886, 47.4609375, 29.725609756097562, 15.75920245398773], bp=1.0, sys_len=22432, ref_len=22317)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZZ2FzI_x2rPt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ]
    },
    {
      "metadata": {
        "id": "G0i4b_woZdRW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1246
        },
        "outputId": "0980dd3e-34ae-4fa8-ba71-1421fc8ddb00"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# !ls ./spa-eng/\n",
        "# !cat ./spa-eng/spa.txt\n",
        "\n",
        "sentences = []\n",
        "\n",
        "sentences = []\n",
        "counter = 0\n",
        "with open(\"./spa-eng/spa.txt\",'r') as f:\n",
        "    for line in f:\n",
        "        counter += 1\n",
        "        if counter == 4001:\n",
        "          break\n",
        "        line = line.split('\\t')\n",
        "        sentences.append((line[1], line[0]))\n",
        "\n",
        "print(sentences[2])\n",
        "\n",
        "\n",
        "print(\"Original:\", sentences[0])\n",
        "sentences = [(preprocess(source), preprocess(target)) for (source, target) in sentences]\n",
        "print(\"Preprocessed:\", sentences[0])\n",
        "\n",
        "source_sentences, target_sentences = list(zip(*sentences))\n",
        "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "source_tokenizer.fit_on_texts(source_sentences)\n",
        "source_data = source_tokenizer.texts_to_sequences(source_sentences)\n",
        "print(\"Sequence:\", source_data[0])\n",
        "source_data = tf.keras.preprocessing.sequence.pad_sequences(source_data, padding='post')\n",
        "print(\"Padded:\", source_data[0])\n",
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "target_tokenizer.fit_on_texts(target_sentences)\n",
        "target_data = target_tokenizer.texts_to_sequences(target_sentences)\n",
        "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post')\n",
        "\n",
        "# Create labels for the decoder by shifting the target sequence\n",
        "# one to the right.\n",
        "target_labels = np.zeros(target_data.shape)\n",
        "target_labels[:,0:target_data.shape[1] -1] = target_data[:,1:]\n",
        "\n",
        "print(\"Target sequence\", target_data[0])\n",
        "print(\"Target label\", target_labels[0])\n",
        "\n",
        "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "def decode(encoded, tokenizer):\n",
        "  for number in encoded:\n",
        "    if number !=0:\n",
        "      print (\"%d -> %s\" % (number, tokenizer.index_word[number]))\n",
        "      \n",
        "decode(source_data[0], source_tokenizer)\n",
        "\n",
        "batch_size = 5\n",
        "dataset = tf.data.Dataset.from_tensor_slices((source_data, target_data, target_labels)).batch(batch_size)\n",
        "\n",
        "example_batch = next(iter(dataset))\n",
        "source, target, taget_labels = example_batch\n",
        "print(\"Shapes:\", source.shape, target.shape, taget_labels.shape)\n",
        "\n",
        "embedding_size = 32\n",
        "rnn_size = 64\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(source_vocab_size,\n",
        "                                               embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)        \n",
        "    return output, state\n",
        "  \n",
        "  def init_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, rnn_size))\n",
        "  \n",
        "  \n",
        "# Create a batch of one sentence\n",
        "ex_sentence = tf.expand_dims(source_data[0], axis=0)\n",
        "ex_translation = tf.expand_dims(target_data[0], axis=0)\n",
        "ex_labels = tf.expand_dims(target_labels[0], axis=0)\n",
        "print(ex_sentence.shape)\n",
        "\n",
        "encoder = Encoder()\n",
        "hidden_state = encoder.init_state(batch_size=1)\n",
        "print(hidden_state.shape)\n",
        "\n",
        "output, hidden_state = encoder(ex_sentence, hidden_state)\n",
        "print(output.shape)\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, \n",
        "                                               embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    logits = self.dense(output)\n",
        "    return logits, state\n",
        "  \n",
        "\n",
        "decoder = Decoder()\n",
        "decoder_output, decoder_state = decoder(ex_labels, hidden_state)\n",
        "print(decoder_output.shape)\n",
        "\n",
        "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def calc_loss(targets, logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  return crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "print(\"Loss\", calc_loss(ex_labels, decoder_output))\n",
        "\n",
        "\n",
        "def translate2(idx=None):\n",
        "  \n",
        "    if idx == None: \n",
        "      idx = np.random.choice(len(sentences))\n",
        "    \n",
        "    input_sent = source_data[idx]\n",
        "    input_sent = tf.expand_dims(input_sent, axis=0)\n",
        "    \n",
        "    hidden_state = encoder.init_state(batch_size=1)\n",
        "    output, hidden_state = encoder(input_sent, hidden_state)\n",
        "    \n",
        "    decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "    out_words = []\n",
        "    \n",
        "    decoder_state = hidden_state\n",
        "\n",
        "    while True:\n",
        "      \n",
        "        decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
        "        decoder_input = tf.argmax(decoder_output, -1)\n",
        "        word_idx = decoder_input.numpy()[0][0]\n",
        "        # if we've predicted 0 (which is reserved, usually this will only happen\n",
        "        # before the decoder is trained, just stop translating and return\n",
        "        # what we have)\n",
        "        if word_idx == 0: \n",
        "          out_words.append('<end>')\n",
        "        else:\n",
        "          out_words.append(target_tokenizer.index_word[word_idx])\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "          break\n",
        "          \n",
        "    translation = ' '.join(out_words)    \n",
        "    return sentences[idx][0], sentences[idx][1], translation\n",
        "  \n",
        "input_sent, target_sent, translation = translate2()\n",
        "print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "@tf.function # remove this annotation when debugging\n",
        "def train_step(source_seq, target_seq, target_labels, initial_state):\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    encoder_output, encoder_state = encoder(source_seq, initial_state)\n",
        "    logits, decoder_state = decoder(target_seq, encoder_state)\n",
        "    loss = calc_loss(target_labels, logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "  \n",
        "    en_initial_states = encoder.init_state(batch_size)\n",
        "    \n",
        "    for batch, (source_seq, target_seq, target_labels) in enumerate(dataset):\n",
        "      loss = train_step(source_seq, target_seq, target_labels, en_initial_states)\n",
        "      elapsed = time.time() - start\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      print(\"Epoch #%d, Loss %.4f, Time %.2f sec\" % (epoch, loss, elapsed))\n",
        "      input_sent, target_sent, translation = translate2()\n",
        "      print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))\n",
        "      \n",
        "      \n",
        "references, hypotheses = [], []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  input_sent, target_sent, translation = translate2()\n",
        "  references.append(target_sent)\n",
        "  hypotheses.append(\"<start> \" + translation)\n",
        "  \n",
        "results = sacrebleu.raw_corpus_bleu(hypotheses, [references])\n",
        "print(results)\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Vaya.\\n', 'Go.')\n",
            "Original: ('Ve.\\n', 'Go.')\n",
            "Preprocessed: ('<start> Ve . <end>', '<start> Go . <end>')\n",
            "Sequence: [1, 62, 3, 2]\n",
            "Padded: [ 1 62  3  2  0  0  0  0  0  0  0  0]\n",
            "Target sequence [ 1 18  3  2  0  0  0  0]\n",
            "Target label [18.  3.  2.  0.  0.  0.  0.  0.]\n",
            "1 -> <start>\n",
            "62 -> ve\n",
            "3 -> .\n",
            "2 -> <end>\n",
            "Shapes: (5, 12) (5, 8) (5, 8)\n",
            "(1, 12)\n",
            "(1, 64)\n",
            "(1, 12, 64)\n",
            "(1, 8, 1108)\n",
            "Loss tf.Tensor(2.630156, shape=(), dtype=float32)\n",
            "Input: <start> Llevate mi coche . <end>\n",
            "Target: <start> Take my car . <end>\n",
            "Translation: feast of bear naked gas feast of , goodbye closed stood agreed way relaxed answer answer went sick dancing twice\n",
            "\n",
            "Epoch #0, Loss 2.4417, Time 7.34 sec\n",
            "Input: <start> Agarra fuertemente . <end>\n",
            "Target: <start> Hang on . <end>\n",
            "Translation: you re a a a . <end>\n",
            "\n",
            "Epoch #10, Loss 1.2688, Time 5.02 sec\n",
            "Input: <start> No pares . <end>\n",
            "Target: <start> Don t stop . <end>\n",
            "Translation: don t be evil . <end>\n",
            "\n",
            "Epoch #20, Loss 0.6649, Time 4.99 sec\n",
            "Input: <start> Deja que Tom se quede . <end>\n",
            "Target: <start> Let Tom stay . <end>\n",
            "Translation: stop talking . <end>\n",
            "\n",
            "Epoch #30, Loss 0.3846, Time 5.03 sec\n",
            "Input: <start> Los ninos corren rapido . <end>\n",
            "Target: <start> Boys run fast . <end>\n",
            "Translation: boys run fast . <end>\n",
            "\n",
            "Epoch #40, Loss 0.1939, Time 5.07 sec\n",
            "Input: <start> No te desconcentres . <end>\n",
            "Target: <start> Stay focused . <end>\n",
            "Translation: stay focused . <end>\n",
            "\n",
            "Epoch #50, Loss 0.1538, Time 5.03 sec\n",
            "Input: <start> Te lo agradezco mucho . <end>\n",
            "Target: <start> Thanks a lot . <end>\n",
            "Translation: thanks a lot . <end>\n",
            "\n",
            "Epoch #60, Loss 0.0828, Time 5.66 sec\n",
            "Input: <start> Estuvo chilo . <end>\n",
            "Target: <start> It was cool . <end>\n",
            "Translation: it was cool . <end>\n",
            "\n",
            "Epoch #70, Loss 0.0432, Time 5.12 sec\n",
            "Input: <start> Estoy mentalizado . <end>\n",
            "Target: <start> I m psyched . <end>\n",
            "Translation: i m drowning . <end>\n",
            "\n",
            "Epoch #80, Loss 0.0347, Time 5.00 sec\n",
            "Input: <start> Como quieras . <end>\n",
            "Target: <start> As you like . <end>\n",
            "Translation: as you live fast . <end>\n",
            "\n",
            "Epoch #90, Loss 0.0214, Time 5.08 sec\n",
            "Input: <start> Me encantan los juegos . <end>\n",
            "Target: <start> I love games . <end>\n",
            "Translation: i love games . <end>\n",
            "\n",
            "BLEU(score=32.42611838065925, counts=[16943, 7974, 4011, 1664], totals=[23494, 19494, 15494, 11494], precisions=[72.11628500893845, 40.904893813481074, 25.887440299470764, 14.477118496606925], bp=1.0, sys_len=23494, ref_len=23140)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yDUohOeB6qZi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ]
    },
    {
      "metadata": {
        "id": "l9EkPnIVLCTk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bc84df73-9185-4d7e-ce00-4d2c4462b175"
      },
      "cell_type": "code",
      "source": [
        "references, hypotheses = [], []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  input_sent, target_sent, translation = translate()\n",
        "  translation, input_sent, translation_back = translate2()\n",
        "  references.append(input_sent)\n",
        "  hypotheses.append(\"<start> \" + translation_back)\n",
        "  \n",
        "results = sacrebleu.raw_corpus_bleu(hypotheses, [references])\n",
        "print(results)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU(score=31.825281588561914, counts=[16893, 7877, 3920, 1610], totals=[23509, 19509, 15509, 11509], precisions=[71.8575864562508, 40.376236608744684, 25.275646398865174, 13.989052046224694], bp=1.0, sys_len=23509, ref_len=23134)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}