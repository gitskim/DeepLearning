{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADL_HW2_part2_final.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "4EeSP28eXL4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e45b3564-daa8-4af9-c6fc-e714c10ab388"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9fDtbNa-CswE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Collect a dataset of at least three landmarks. Your dataset should include at least 100 images of each in train, 50 in validation, and 25 in test (using more images is fine). You can randomly shuffle your dataset to create these splits.\n",
        "\n",
        "3 labels: Central Park, Empire State Building, Statue of Liberty\n",
        "\n",
        "I scraped approximately 100 images each from Google. "
      ]
    },
    {
      "metadata": {
        "id": "4JLb2-3LXPrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# The code in this notebook should work identically between TF v1 and v2\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16, MobileNet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6AbGnliEXU4q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ef579a51-c162-4166-8599-33f8c68cb324"
      },
      "cell_type": "code",
      "source": [
        "# Images will be resized to(TARGET_SHAPE, TARGET_SHAPE) as they're read off disk.\n",
        "TARGET_SHAPE = 150 \n",
        "BATCH_SIZE = 5\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "val_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(directory=\"/content/gdrive/My Drive/img/train\", \n",
        "                                                     shuffle=True, # Best practice: shuffle the training data\n",
        "                                                     target_size=(TARGET_SHAPE, TARGET_SHAPE),\n",
        "                                                     class_mode='categorical')\n",
        "\n",
        "val_data_gen = val_image_generator.flow_from_directory(directory=\"/content/gdrive/My Drive/img/val\", \n",
        "                                                     shuffle=True, # Best practice: shuffle the training data\n",
        "                                                     target_size=(TARGET_SHAPE, TARGET_SHAPE),\n",
        "                                                     class_mode='categorical')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 228 images belonging to 3 classes.\n",
            "Found 59 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dxakIzkAEsuM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 Next, how small of a model (in terms of the number of parameters) can you write to classify these images reasonably well? \n",
        "\n",
        "I tried 4 sets of convolution and maxpooling layers in the beginning, which gave me the validation accuracy of about 80%. Then I made the model even smaller with two sets of convolution and maxpooling layers, which resulted in ~90% validation accruacy. Then I tried one set, which yielded less than 40% accruacy. The smallest model I could achieve was two sets of convolution and maxpooling layers. I belive with too many sets of layers,the small nubmer of data I have causes overfitting, which gave me the lower accruacy in the first place ."
      ]
    },
    {
      "metadata": {
        "id": "8tnN0bKOXl9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "b2305290-4013-46e5-8bb1-35891c8f77ae"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 7\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(233 / float(BATCH_SIZE))),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(60 / float(BATCH_SIZE)))\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "1/2 [==============>...............] - ETA: 9s - loss: 0.9241 - acc: 0.7500"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:885: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 17s 8s/step - loss: 0.9376 - acc: 0.6441\n",
            "7/7 [==============================] - 77s 11s/step - loss: 1.0901 - acc: 0.4144 - val_loss: 0.9376 - val_acc: 0.6441\n",
            "Epoch 2/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.7277 - acc: 0.5763\n",
            "7/7 [==============================] - 19s 3s/step - loss: 0.8503 - acc: 0.6216 - val_loss: 0.7277 - val_acc: 0.5763\n",
            "Epoch 3/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6622 - acc: 0.6949\n",
            "7/7 [==============================] - 22s 3s/step - loss: 0.8125 - acc: 0.6036 - val_loss: 0.6622 - val_acc: 0.6949\n",
            "Epoch 4/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6930 - acc: 0.7119\n",
            "7/7 [==============================] - 22s 3s/step - loss: 0.7113 - acc: 0.6667 - val_loss: 0.6930 - val_acc: 0.7119\n",
            "Epoch 5/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6129 - acc: 0.6610\n",
            "7/7 [==============================] - 22s 3s/step - loss: 0.6618 - acc: 0.7072 - val_loss: 0.6129 - val_acc: 0.6610\n",
            "Epoch 6/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4313 - acc: 0.8305\n",
            "7/7 [==============================] - 23s 3s/step - loss: 0.5358 - acc: 0.7973 - val_loss: 0.4313 - val_acc: 0.8305\n",
            "Epoch 7/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5416 - acc: 0.7966\n",
            "7/7 [==============================] - 22s 3s/step - loss: 0.4507 - acc: 0.8153 - val_loss: 0.5416 - val_acc: 0.7966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "atk7vaURBw08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "02b5c592-601e-4668-eb69-698bafdddaf9"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 7\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(233 / float(BATCH_SIZE))),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(60 / float(BATCH_SIZE)))\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "7/8 [=========================>....] - ETA: 2s - loss: 2.0362 - acc: 0.3469"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:885: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 4s 2s/step - loss: 0.9917 - acc: 0.3559\n",
            "8/8 [==============================] - 21s 3s/step - loss: 1.8689 - acc: 0.3509 - val_loss: 0.9917 - val_acc: 0.3559\n",
            "Epoch 2/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6990 - acc: 0.7119\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.8892 - acc: 0.5746 - val_loss: 0.6990 - val_acc: 0.7119\n",
            "Epoch 3/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6519 - acc: 0.6780\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.5906 - acc: 0.7412 - val_loss: 0.6519 - val_acc: 0.6780\n",
            "Epoch 4/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5016 - acc: 0.8136\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.4523 - acc: 0.7939 - val_loss: 0.5016 - val_acc: 0.8136\n",
            "Epoch 5/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4960 - acc: 0.8136\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.3174 - acc: 0.8596 - val_loss: 0.4960 - val_acc: 0.8136\n",
            "Epoch 6/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3590 - acc: 0.8983\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.1998 - acc: 0.9298 - val_loss: 0.3590 - val_acc: 0.8983\n",
            "Epoch 7/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2183 - acc: 0.8983\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.1265 - acc: 0.9605 - val_loss: 0.2183 - val_acc: 0.8983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8VjMKWTYCc_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "d41ffb77-abd4-456d-8870-c8ae0cc1ff31"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 7\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(233 / float(BATCH_SIZE))),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(60 / float(BATCH_SIZE)))\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "7/8 [=========================>....] - ETA: 1s - loss: 9.6559 - acc: 0.3163"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:885: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 4s 2s/step - loss: 10.4283 - acc: 0.3390\n",
            "8/8 [==============================] - 16s 2s/step - loss: 9.9310 - acc: 0.3070 - val_loss: 10.4283 - val_acc: 0.3390\n",
            "Epoch 2/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 10.5682 - acc: 0.3390\n",
            "8/8 [==============================] - 16s 2s/step - loss: 10.6119 - acc: 0.3246 - val_loss: 10.5682 - val_acc: 0.3390\n",
            "Epoch 3/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 10.8014 - acc: 0.3390\n",
            "8/8 [==============================] - 16s 2s/step - loss: 10.6119 - acc: 0.3246 - val_loss: 10.8014 - val_acc: 0.3390\n",
            "Epoch 4/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 10.5682 - acc: 0.3390\n",
            "8/8 [==============================] - 15s 2s/step - loss: 11.0036 - acc: 0.3246 - val_loss: 10.5682 - val_acc: 0.3390\n",
            "Epoch 5/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 10.7081 - acc: 0.3390\n",
            "8/8 [==============================] - 16s 2s/step - loss: 11.0036 - acc: 0.3246 - val_loss: 10.7081 - val_acc: 0.3390\n",
            "Epoch 6/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 10.6614 - acc: 0.3390\n",
            "8/8 [==============================] - 15s 2s/step - loss: 10.6119 - acc: 0.3246 - val_loss: 10.6614 - val_acc: 0.3390\n",
            "Epoch 7/7\n",
            "2/2 [==============================] - 4s 2s/step - loss: 10.6148 - acc: 0.3390\n",
            "8/8 [==============================] - 15s 2s/step - loss: 11.0036 - acc: 0.3246 - val_loss: 10.6148 - val_acc: 0.3390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OiC8qI0HDgj7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Write a model to classify your dataset using transfer learning. Run an experiment and report your results. What do you find?\n",
        "\n",
        "After setting the VGG16 convolutional base not trainable, I added a simple 2-layer model of 64-node dense layer and 3-node dense layer. I trained with an adam optimizer and cross entropy loss function. I found that my model is doing pretty well, because validation accruacy is about 95%, and it started with a pretty high validation accuracy of 85%. My reasoning of the high validation accuracy is because from the transfer learning, the model already learned basics of images such as edges, shapes, so it didn't take long for models to learn about my dataset. "
      ]
    },
    {
      "metadata": {
        "id": "rImpl84GZSvV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "99dad0e5-6305-4ae5-f090-b1eb7599b4ae"
      },
      "cell_type": "code",
      "source": [
        "conv_base = VGG16(weights='imagenet',include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  loss= 'categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(233 / float(BATCH_SIZE))),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(60 / float(BATCH_SIZE)))\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Epoch 1/7\n",
            "6/7 [========================>.....] - ETA: 8s - loss: 1.0665 - acc: 0.5895 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:885: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 20s 10s/step - loss: 0.4504 - acc: 0.8475\n",
            "7/7 [==============================] - 81s 12s/step - loss: 1.0206 - acc: 0.6081 - val_loss: 0.4504 - val_acc: 0.8475\n",
            "Epoch 2/7\n",
            "2/2 [==============================] - 19s 10s/step - loss: 0.2166 - acc: 0.9322\n",
            "7/7 [==============================] - 75s 11s/step - loss: 0.3321 - acc: 0.9144 - val_loss: 0.2166 - val_acc: 0.9322\n",
            "Epoch 3/7\n",
            "2/2 [==============================] - 20s 10s/step - loss: 0.1833 - acc: 0.9322\n",
            "7/7 [==============================] - 74s 11s/step - loss: 0.1984 - acc: 0.9279 - val_loss: 0.1833 - val_acc: 0.9322\n",
            "Epoch 4/7\n",
            "2/2 [==============================] - 20s 10s/step - loss: 0.1157 - acc: 0.9492\n",
            "7/7 [==============================] - 74s 11s/step - loss: 0.1053 - acc: 0.9640 - val_loss: 0.1157 - val_acc: 0.9492\n",
            "Epoch 5/7\n",
            "2/2 [==============================] - 19s 10s/step - loss: 0.1549 - acc: 0.9153\n",
            "7/7 [==============================] - 74s 11s/step - loss: 0.0609 - acc: 0.9865 - val_loss: 0.1549 - val_acc: 0.9153\n",
            "Epoch 6/7\n",
            "2/2 [==============================] - 19s 10s/step - loss: 0.1282 - acc: 0.9492\n",
            "7/7 [==============================] - 74s 11s/step - loss: 0.0417 - acc: 0.9955 - val_loss: 0.1282 - val_acc: 0.9492\n",
            "Epoch 7/7\n",
            "2/2 [==============================] - 19s 10s/step - loss: 0.0947 - acc: 0.9492\n",
            "7/7 [==============================] - 74s 11s/step - loss: 0.0303 - acc: 1.0000 - val_loss: 0.0947 - val_acc: 0.9492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gv2bkVE5E0pQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 Explore the available pretrained ​models​, and see if any are suitable. Run an experiment and report your results.\n",
        "\n",
        "I tried VGG16 and MobileNet for pretrained models. MobileNet's validation accuracy is less than 70%, but VGG16's was 95%. I believe the reason is that MobileNet is built for mobile devices so it is mindful of the restricted resources. However, VGG16 trained on a less resource-restricted environment. "
      ]
    },
    {
      "metadata": {
        "id": "YwSNt4g-eTpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "d8264c04-e908-478e-dded-a25c5a1857f8"
      },
      "cell_type": "code",
      "source": [
        "# Images will be resized to(TARGET_SHAPE, TARGET_SHAPE) as they're read off disk.\n",
        "TARGET_SHAPE = 224 \n",
        "BATCH_SIZE = 5\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "val_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(directory=\"/content/gdrive/My Drive/img/train\", \n",
        "                                                     shuffle=True, # Best practice: shuffle the training data\n",
        "                                                     target_size=(TARGET_SHAPE, TARGET_SHAPE),\n",
        "                                                     class_mode='categorical')\n",
        "\n",
        "val_data_gen = val_image_generator.flow_from_directory(directory=\"/content/gdrive/My Drive/img/val\", \n",
        "                                                     shuffle=True, # Best practice: shuffle the training data\n",
        "                                                     target_size=(TARGET_SHAPE, TARGET_SHAPE),\n",
        "                                                     class_mode='categorical')\n",
        "\n",
        "conv_base = MobileNet(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  loss= 'categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(233 / float(BATCH_SIZE))),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(60 / float(BATCH_SIZE)))\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 228 images belonging to 3 classes.\n",
            "Found 59 images belonging to 3 classes.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "Epoch 1/7\n",
            "7/8 [=========================>....] - ETA: 3s - loss: 7.0896 - acc: 0.5204"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:885: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 10s 5s/step - loss: 7.1586 - acc: 0.5085\n",
            "8/8 [==============================] - 41s 5s/step - loss: 6.7448 - acc: 0.5439 - val_loss: 7.1586 - val_acc: 0.5085\n",
            "Epoch 2/7\n",
            "2/2 [==============================] - 10s 5s/step - loss: 5.5456 - acc: 0.6441\n",
            "8/8 [==============================] - 36s 5s/step - loss: 7.0424 - acc: 0.5746 - val_loss: 5.5456 - val_acc: 0.6441\n",
            "Epoch 3/7\n",
            "2/2 [==============================] - 11s 6s/step - loss: 6.2977 - acc: 0.6102\n",
            "8/8 [==============================] - 38s 5s/step - loss: 5.4912 - acc: 0.6360 - val_loss: 6.2977 - val_acc: 0.6102\n",
            "Epoch 4/7\n",
            "2/2 [==============================] - 10s 5s/step - loss: 6.3987 - acc: 0.6102\n",
            "8/8 [==============================] - 34s 4s/step - loss: 5.9196 - acc: 0.6447 - val_loss: 6.3987 - val_acc: 0.6102\n",
            "Epoch 5/7\n",
            "2/2 [==============================] - 10s 5s/step - loss: 5.8154 - acc: 0.6271\n",
            "8/8 [==============================] - 39s 5s/step - loss: 5.4468 - acc: 0.6447 - val_loss: 5.8154 - val_acc: 0.6271\n",
            "Epoch 6/7\n",
            "2/2 [==============================] - 9s 5s/step - loss: 5.4459 - acc: 0.6441\n",
            "8/8 [==============================] - 36s 4s/step - loss: 5.0116 - acc: 0.6491 - val_loss: 5.4459 - val_acc: 0.6441\n",
            "Epoch 7/7\n",
            "2/2 [==============================] - 10s 5s/step - loss: 5.3167 - acc: 0.6610\n",
            "8/8 [==============================] - 37s 5s/step - loss: 6.3918 - acc: 0.6667 - val_loss: 5.3167 - val_acc: 0.6610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fyiGMPcVFLjv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}